{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50c33c7",
   "metadata": {},
   "source": [
    "# Setup & Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9021be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# fondation model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google-genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32d2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4092f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store =  Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24fbe8",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802aa960",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cfcc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/ai-engineering-book.pdf\"\n",
    "loader  = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97c49110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'calibre 7.4.0', 'creator': 'calibre 7.4.0', 'creationdate': '2024-12-07T18:13:04+00:00', 'author': 'Chip Huyen', 'moddate': '2024-12-07T18:13:04+00:00', 'title': 'AI Engineering (for True Epub)', 'source': '../data/ai-engineering-book.pdf', 'total_pages': 991, 'page': 10, 'page_label': '11'}, page_content='I especially learned from early readers who tested my assumptions,\\nintroduced me to different perspectives, and exposed me to new problems\\nand approaches. Some sections of the book have also received thousands of\\ncomments from the community after being shared on my blog, many giving\\nme new perspectives or confirming a hypothesis.\\nI hope that this learning process will continue for me now that the book is in\\nyour hands, as you have experiences and perspectives that are unique to\\nyou. Please feel free to share any feedback you might have for this book\\nwith me via X, LinkedIn, or email at hi@huyenchip.com.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d387ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I especially learned from early readers who tested my assumptions,\n",
      "introduced me to different perspectives, and exposed me to new problems\n",
      "and approaches. Some sections of the book have also received thousands of\n",
      "comments from the community after being shared on my blog, many giving\n",
      "me new perspectives or confirming a hypothesis.\n",
      "I hope that this learning process will continue for me now that the book is in\n",
      "your hands, as you have experiences and perspectives that are unique to\n",
      "you. Please feel free to share any feedback you might have for this book\n",
      "with me via X, LinkedIn, or email at hi@huyenchip.com.\n"
     ]
    }
   ],
   "source": [
    "print(docs[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093710a2",
   "metadata": {},
   "source": [
    "## Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73c181f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 1640 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccac730",
   "metadata": {},
   "source": [
    "## Storing Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ca9385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8047d70e-5f18-45d6-bf89-0618bcba0ca1', 'aa99f2ef-1b95-4b02-8dcc-fbf60ed40eea', 'e87f17cf-2246-4255-b3ba-b4b775b49d6a']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060df29",
   "metadata": {},
   "source": [
    "# Retrieval & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1ec8518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: Where is the Eiffel Tower located? \n",
      "Context: I am cool \n",
      "Answer:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define your custom template\n",
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Example usage\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"I am cool\", \"question\": \"Where is the Eiffel Tower located?\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38ee429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6612316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state['question'])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eabfbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "765dd483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 44.769214598s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 42.645686886s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 38.530838157s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='667a9711-30a5-4486-b718-fe194846c2b3', metadata={'author': 'Chip Huyen', 'creationdate': '2024-12-07T18:13:04+00:00', 'moddate': '2024-12-07T18:13:04+00:00', 'page_label': '242', 'title': 'AI Engineering (for True Epub)', 'total_pages': 991, 'page': 241, 'producer': 'calibre 7.4.0', 'creator': 'calibre 7.4.0', 'source': '../data/ai-engineering-book.pdf'}, page_content='on your production data. In general, the closer your data is to a model’s\\ntraining data, the better the model can perform on your data.\\nCompared to the rest of the book, this section is math-heavy. If you find it\\nconfusing, feel free to skip the math part and focus on the discussion of how\\nto interpret these metrics. Even if you’re not training or finetuning language\\nmodels, understanding these metrics can help with evaluating which models\\nto use for your application. These metrics can occasionally be used for\\ncertain evaluation and data deduplication techniques, as discussed\\nthroughout this book.\\nEntropy\\nEntropy measures how much information, on average, a token carries. The\\nhigher the entropy, the more information each token carries, and the more\\nbits are needed to represent a token.\\nLet’s use a simple example to illustrate this. Imagine you want to create a\\nlanguage to describe positions within a square, as shown in Figure 3-4. If'), Document(id='b78e1a0f-479a-4850-9e2c-7955807545b3', metadata={'page_label': '243', 'title': 'AI Engineering (for True Epub)', 'creator': 'calibre 7.4.0', 'page': 242, 'producer': 'calibre 7.4.0', 'moddate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'author': 'Chip Huyen', 'creationdate': '2024-12-07T18:13:04+00:00', 'source': '../data/ai-engineering-book.pdf'}, page_content='Figure 3-4. Two languages describe positions within a square. Compared to the language on the left\\n(a), the tokens on the right (b) carry more information, but they need more bits to represent them.\\nIf your language has four tokens, shown as (b) in Figure 3-4, each token can\\ngive you a more specific position: upper-left, upper-right, lower-left, or\\nlower-right. However, since there are now four tokens, you need two bits to\\nrepresent them. The entropy of this language is 2. This language has higher\\nentropy, since each token carries more information, but each token requires\\nmore bits to represent.\\nIntuitively, entropy measures how difficult it is to predict what comes next\\nin a language. The lower a language’s entropy (the less information a token\\nof a language carries), the more predictable that language. In our previous\\nexample, the language with only two tokens is easier to predict than the\\nlanguage with four (you have to predict among only two possible tokens'), Document(id='9f8746b2-97c6-4ba7-af99-e5e6b0b85145', metadata={'title': 'AI Engineering (for True Epub)', 'author': 'Chip Huyen', 'source': '../data/ai-engineering-book.pdf', 'page_label': '246', 'page': 245, 'creator': 'calibre 7.4.0', 'total_pages': 991, 'producer': 'calibre 7.4.0', 'moddate': '2024-12-07T18:13:04+00:00', 'creationdate': '2024-12-07T18:13:04+00:00'}, page_content='perplexity is defined as:\\nPPL(P)=2H(P)\\nThe perplexity of a language model (with the learned distribution Q) on this\\ndataset is defined as:\\nPPL(P,Q)=2H(P,Q)\\nIf cross entropy measures how difficult it is for a model to predict the next\\ntoken, perplexity measures the amount of uncertainty it has when predicting'), Document(id='8335c0c2-053b-4516-90e7-62a3705583c4', metadata={'author': 'Chip Huyen', 'creator': 'calibre 7.4.0', 'source': '../data/ai-engineering-book.pdf', 'creationdate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'page_label': '667', 'page': 666, 'producer': 'calibre 7.4.0', 'title': 'AI Engineering (for True Epub)', 'moddate': '2024-12-07T18:13:04+00:00'}, page_content='copies together into one new base model that contains the learning of all\\nconstituent models.\\nThe idea of combining models together to obtain better performance started\\nwith model ensemble methods. According to Wikipedia, ensembling\\ncombines “multiple learning algorithms to obtain better predictive\\nperformance than could be obtained from any of the constituent learning\\nalgorithms alone.” If model merging typically involves mixing parameters\\nof constituent models together, ensembling typically combines only model\\noutputs while keeping each constituent model intact.\\nFor example, in ensembling, given a query, you might use three models to\\ngenerate three different answers. Then, a final answer is generated based on\\nthese three answers, using a simple majority vote or another trainable ML\\nmodule. While ensembling can generally improve performance, it has a\\nhigher inference cost since it requires multiple inference calls per request.')]\n",
      "\n",
      "\n",
      "Answer: Entropy measures how much information, on average, a token carries. The higher the entropy, the more information each token carries, and the more bits are needed to represent a token. Intuitively, it also measures how difficult it is to predict what comes next in a language.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is entrophy?\"})\n",
    "\n",
    "print(f\"Context: {result[\"context\"]}\\n\\n\")\n",
    "print(f\"Answer: {result[\"answer\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0a4a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='667a9711-30a5-4486-b718-fe194846c2b3', metadata={'creator': 'calibre 7.4.0', 'source': '../data/ai-engineering-book.pdf', 'creationdate': '2024-12-07T18:13:04+00:00', 'title': 'AI Engineering (for True Epub)', 'author': 'Chip Huyen', 'page_label': '242', 'page': 241, 'moddate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'producer': 'calibre 7.4.0'}, page_content='on your production data. In general, the closer your data is to a model’s\\ntraining data, the better the model can perform on your data.\\nCompared to the rest of the book, this section is math-heavy. If you find it\\nconfusing, feel free to skip the math part and focus on the discussion of how\\nto interpret these metrics. Even if you’re not training or finetuning language\\nmodels, understanding these metrics can help with evaluating which models\\nto use for your application. These metrics can occasionally be used for\\ncertain evaluation and data deduplication techniques, as discussed\\nthroughout this book.\\nEntropy\\nEntropy measures how much information, on average, a token carries. The\\nhigher the entropy, the more information each token carries, and the more\\nbits are needed to represent a token.\\nLet’s use a simple example to illustrate this. Imagine you want to create a\\nlanguage to describe positions within a square, as shown in Figure 3-4. If'), Document(id='b78e1a0f-479a-4850-9e2c-7955807545b3', metadata={'producer': 'calibre 7.4.0', 'title': 'AI Engineering (for True Epub)', 'creator': 'calibre 7.4.0', 'moddate': '2024-12-07T18:13:04+00:00', 'creationdate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'page_label': '243', 'page': 242, 'source': '../data/ai-engineering-book.pdf', 'author': 'Chip Huyen'}, page_content='Figure 3-4. Two languages describe positions within a square. Compared to the language on the left\\n(a), the tokens on the right (b) carry more information, but they need more bits to represent them.\\nIf your language has four tokens, shown as (b) in Figure 3-4, each token can\\ngive you a more specific position: upper-left, upper-right, lower-left, or\\nlower-right. However, since there are now four tokens, you need two bits to\\nrepresent them. The entropy of this language is 2. This language has higher\\nentropy, since each token carries more information, but each token requires\\nmore bits to represent.\\nIntuitively, entropy measures how difficult it is to predict what comes next\\nin a language. The lower a language’s entropy (the less information a token\\nof a language carries), the more predictable that language. In our previous\\nexample, the language with only two tokens is easier to predict than the\\nlanguage with four (you have to predict among only two possible tokens'), Document(id='9f8746b2-97c6-4ba7-af99-e5e6b0b85145', metadata={'total_pages': 991, 'source': '../data/ai-engineering-book.pdf', 'producer': 'calibre 7.4.0', 'page': 245, 'author': 'Chip Huyen', 'creator': 'calibre 7.4.0', 'moddate': '2024-12-07T18:13:04+00:00', 'title': 'AI Engineering (for True Epub)', 'page_label': '246', 'creationdate': '2024-12-07T18:13:04+00:00'}, page_content='perplexity is defined as:\\nPPL(P)=2H(P)\\nThe perplexity of a language model (with the learned distribution Q) on this\\ndataset is defined as:\\nPPL(P,Q)=2H(P,Q)\\nIf cross entropy measures how difficult it is for a model to predict the next\\ntoken, perplexity measures the amount of uncertainty it has when predicting'), Document(id='8335c0c2-053b-4516-90e7-62a3705583c4', metadata={'moddate': '2024-12-07T18:13:04+00:00', 'page_label': '667', 'title': 'AI Engineering (for True Epub)', 'producer': 'calibre 7.4.0', 'total_pages': 991, 'creationdate': '2024-12-07T18:13:04+00:00', 'source': '../data/ai-engineering-book.pdf', 'author': 'Chip Huyen', 'creator': 'calibre 7.4.0', 'page': 666}, page_content='copies together into one new base model that contains the learning of all\\nconstituent models.\\nThe idea of combining models together to obtain better performance started\\nwith model ensemble methods. According to Wikipedia, ensembling\\ncombines “multiple learning algorithms to obtain better predictive\\nperformance than could be obtained from any of the constituent learning\\nalgorithms alone.” If model merging typically involves mixing parameters\\nof constituent models together, ensembling typically combines only model\\noutputs while keeping each constituent model intact.\\nFor example, in ensembling, given a query, you might use three models to\\ngenerate three different answers. Then, a final answer is generated based on\\nthese three answers, using a simple majority vote or another trainable ML\\nmodule. While ensembling can generally improve performance, it has a\\nhigher inference cost since it requires multiple inference calls per request.')]}}\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:332\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    329\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    330\u001b[39m     continuation, client_call_details, request\n\u001b[32m    331\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\grpc\\_channel.py:440\u001b[39m, in \u001b[36m_InactiveRpcError.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1192\u001b[39m state, call = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1193\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1194\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1009\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"The model is overloaded. Please try again later.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2404:6800:4003:c03::5f%5D:443 {grpc_message:\"The model is overloaded. Please try again later.\", grpc_status:14}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mServiceUnavailable\u001b[39m: 503 The model is overloaded. Please try again later.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is entrophy?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstep\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m-------------------\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2657\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2656\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2657\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2664\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2667\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      6\u001b[39m docs_content = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m state[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m messages = prompt.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: docs_content})\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: response.content}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1676\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1673\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1676\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1023\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m     **kwargs: Any,\n\u001b[32m   1021\u001b[39m ) -> LLMResult:\n\u001b[32m   1022\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:840\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    838\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    839\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m         )\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    848\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1089\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1087\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1093\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1790\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:238\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    231\u001b[39m params = (\n\u001b[32m    232\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:208\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlocation is not supported\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc.message:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:869\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Christian\\Programming_Project\\rag-app\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:167\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m next_sleep = _retry_error_helper(\n\u001b[32m    157\u001b[39m     exc,\n\u001b[32m    158\u001b[39m     deadline,\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     timeout,\n\u001b[32m    165\u001b[39m )\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_sleep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What is entrophy?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n-------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e345712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a15ee00c",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph \n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7afb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_response(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state['messages'])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for  message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format intro prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question answering task. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state['messages']\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    #  Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054dc9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_response)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_response\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_response\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"}\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac107343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a412741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hello\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "input_messages = \"hello\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_messages}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Waht is enthropy?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (5a8282b9-56dd-45de-85b4-326af4131f52)\n",
      " Call ID: 5a8282b9-56dd-45de-85b4-326af4131f52\n",
      "  Args:\n",
      "    query: entropy\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'producer': 'calibre 7.4.0', 'creationdate': '2024-12-07T18:13:04+00:00', 'title': 'AI Engineering (for True Epub)', 'author': 'Chip Huyen', 'total_pages': 991, 'page_label': '243', 'moddate': '2024-12-07T18:13:04+00:00', 'page': 242, 'creator': 'calibre 7.4.0', 'source': '../data/ai-engineering-book.pdf'}\n",
      "Content: Figure 3-4. Two languages describe positions within a square. Compared to the language on the left\n",
      "(a), the tokens on the right (b) carry more information, but they need more bits to represent them.\n",
      "If your language has four tokens, shown as (b) in Figure 3-4, each token can\n",
      "give you a more specific position: upper-left, upper-right, lower-left, or\n",
      "lower-right. However, since there are now four tokens, you need two bits to\n",
      "represent them. The entropy of this language is 2. This language has higher\n",
      "entropy, since each token carries more information, but each token requires\n",
      "more bits to represent.\n",
      "Intuitively, entropy measures how difficult it is to predict what comes next\n",
      "in a language. The lower a language’s entropy (the less information a token\n",
      "of a language carries), the more predictable that language. In our previous\n",
      "example, the language with only two tokens is easier to predict than the\n",
      "language with four (you have to predict among only two possible tokens\n",
      "\n",
      "Source: {'creationdate': '2024-12-07T18:13:04+00:00', 'moddate': '2024-12-07T18:13:04+00:00', 'page': 311, 'title': 'AI Engineering (for True Epub)', 'total_pages': 991, 'author': 'Chip Huyen', 'creator': 'calibre 7.4.0', 'page_label': '312', 'producer': 'calibre 7.4.0', 'source': '../data/ai-engineering-book.pdf'}\n",
      "Content: translated into binary digits (0 or 1) in the most efficient way, the entropy is the average number of\n",
      "binary digits required per letter of the original language.”\n",
      " One reason many people might prefer natural log over log base 2 is because natural log has certain\n",
      "properties that makes its math easier. For example, the derivative of natural log ln(x) is 1/x.\n",
      " If you’re unsure what SFT (supervised finetuning) and RLHF (reinforcement learning from human\n",
      "feedback) mean, revisit Chapter 2.\n",
      " Quantization is discussed in Chapter 7.\n",
      " The challenge is that while many complex tasks have measurable objectives, AI isn’t quite good\n",
      "enough to perform complex tasks end-to-end, so AI might be used to do part of the solution.\n",
      "Sometimes, evaluating a part of a solution is harder than evaluating the end outcome. Imagine you\n",
      "want to evaluate someone’s ability to play chess. It’s easier to evaluate the end game outcome\n",
      "(win/lose/draw) than to evaluate just one move.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Entropy measures how difficult it is to predict what comes next in a language. A language with higher entropy means each token carries more information but requires more bits to represent. More formally, entropy is the average number of binary digits required per letter of the original language when translated most efficiently into binary.\n"
     ]
    }
   ],
   "source": [
    "input_messages = \"Waht is enthropy?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_messages}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff55b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba5f6430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is enthropy?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (97fcc165-8d56-485d-a41f-381834c16590)\n",
      " Call ID: 97fcc165-8d56-485d-a41f-381834c16590\n",
      "  Args:\n",
      "    query: entropy\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'creator': 'calibre 7.4.0', 'page': 242, 'moddate': '2024-12-07T18:13:04+00:00', 'author': 'Chip Huyen', 'title': 'AI Engineering (for True Epub)', 'producer': 'calibre 7.4.0', 'total_pages': 991, 'source': '../data/ai-engineering-book.pdf', 'creationdate': '2024-12-07T18:13:04+00:00', 'page_label': '243'}\n",
      "Content: Figure 3-4. Two languages describe positions within a square. Compared to the language on the left\n",
      "(a), the tokens on the right (b) carry more information, but they need more bits to represent them.\n",
      "If your language has four tokens, shown as (b) in Figure 3-4, each token can\n",
      "give you a more specific position: upper-left, upper-right, lower-left, or\n",
      "lower-right. However, since there are now four tokens, you need two bits to\n",
      "represent them. The entropy of this language is 2. This language has higher\n",
      "entropy, since each token carries more information, but each token requires\n",
      "more bits to represent.\n",
      "Intuitively, entropy measures how difficult it is to predict what comes next\n",
      "in a language. The lower a language’s entropy (the less information a token\n",
      "of a language carries), the more predictable that language. In our previous\n",
      "example, the language with only two tokens is easier to predict than the\n",
      "language with four (you have to predict among only two possible tokens\n",
      "\n",
      "Source: {'author': 'Chip Huyen', 'producer': 'calibre 7.4.0', 'creator': 'calibre 7.4.0', 'page': 311, 'title': 'AI Engineering (for True Epub)', 'moddate': '2024-12-07T18:13:04+00:00', 'creationdate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'page_label': '312', 'source': '../data/ai-engineering-book.pdf'}\n",
      "Content: translated into binary digits (0 or 1) in the most efficient way, the entropy is the average number of\n",
      "binary digits required per letter of the original language.”\n",
      " One reason many people might prefer natural log over log base 2 is because natural log has certain\n",
      "properties that makes its math easier. For example, the derivative of natural log ln(x) is 1/x.\n",
      " If you’re unsure what SFT (supervised finetuning) and RLHF (reinforcement learning from human\n",
      "feedback) mean, revisit Chapter 2.\n",
      " Quantization is discussed in Chapter 7.\n",
      " The challenge is that while many complex tasks have measurable objectives, AI isn’t quite good\n",
      "enough to perform complex tasks end-to-end, so AI might be used to do part of the solution.\n",
      "Sometimes, evaluating a part of a solution is harder than evaluating the end outcome. Imagine you\n",
      "want to evaluate someone’s ability to play chess. It’s easier to evaluate the end game outcome\n",
      "(win/lose/draw) than to evaluate just one move.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Entropy intuitively measures how difficult it is to predict what comes next in a language. A language with higher entropy means each token carries more information and requires more bits to represent. More formally, entropy is the average number of binary digits needed per letter of the original language when translated in the most efficient way.\n"
     ]
    }
   ],
   "source": [
    "input_messages = \"What is enthropy?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_messages}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2246dc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Is there any kind of things that similar to it?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (bdc289de-7f3d-4f8a-8eb0-a6785a307288)\n",
      " Call ID: bdc289de-7f3d-4f8a-8eb0-a6785a307288\n",
      "  Args:\n",
      "    query: information content unpredictability bits information representation\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'moddate': '2024-12-07T18:13:04+00:00', 'creator': 'calibre 7.4.0', 'author': 'Chip Huyen', 'total_pages': 991, 'creationdate': '2024-12-07T18:13:04+00:00', 'title': 'AI Engineering (for True Epub)', 'page': 241, 'page_label': '242', 'source': '../data/ai-engineering-book.pdf', 'producer': 'calibre 7.4.0'}\n",
      "Content: bits are needed to represent a token.\n",
      "Let’s use a simple example to illustrate this. Imagine you want to create a\n",
      "language to describe positions within a square, as shown in Figure 3-4. If\n",
      "your language has only two tokens, shown as (a) in Figure 3-4, each token\n",
      "can tell you whether the position is upper or lower. Since there are only two\n",
      "tokens, one bit is sufficient to represent them. The entropy of this language\n",
      "is, therefore, 1.\n",
      "7\n",
      "\n",
      "Source: {'author': 'Chip Huyen', 'total_pages': 991, 'moddate': '2024-12-07T18:13:04+00:00', 'page': 973, 'source': '../data/ai-engineering-book.pdf', 'page_label': '974', 'creator': 'calibre 7.4.0', 'title': 'AI Engineering (for True Epub)', 'producer': 'calibre 7.4.0', 'creationdate': '2024-12-07T18:13:04+00:00'}\n",
      "Content: probabilistic nature of AI, The Probabilistic Nature of AI-Hallucination\n",
      "hallucination, Hallucination-Hallucination\n",
      "inconsistency, Inconsistency-Inconsistency\n",
      "probabilistic definition, The Probabilistic Nature of AI-Hallucination\n",
      "procedural generation, Traditional Data Synthesis Techniques-\n",
      "Simulation\n",
      "product quantization, Embedding-based retrieval\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the provided context, there is no mention of concepts similar to entropy. The text explains what entropy is in terms of bits needed to represent a token and its relation to predictability.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Is there any kind of things that similar to it?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f05f5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you give others term beside enthropy that I need to know?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (489f69b3-8fc3-4061-8ef7-b6ade23791b0)\n",
      " Call ID: 489f69b3-8fc3-4061-8ef7-b6ade23791b0\n",
      "  Args:\n",
      "    query: information theory terms AI engineering\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'author': 'Chip Huyen', 'creator': 'calibre 7.4.0', 'producer': 'calibre 7.4.0', 'moddate': '2024-12-07T18:13:04+00:00', 'page_label': '115', 'source': '../data/ai-engineering-book.pdf', 'title': 'AI Engineering (for True Epub)', 'total_pages': 991, 'page': 114, 'creationdate': '2024-12-07T18:13:04+00:00'}\n",
      "Content: AI engineering. However, AI engineering also brings with it new challenges\n",
      "and solutions. The last section of the chapter discusses the AI engineering\n",
      "stack, including how it has changed from ML engineering.\n",
      "One aspect of AI engineering that is especially challenging to capture in\n",
      "writing is the incredible amount of collective energy, creativity, and\n",
      "engineering talent that the community brings. This collective enthusiasm\n",
      "can often be overwhelming, as it’s impossible to keep up-to-date with new\n",
      "techniques, discoveries, and engineering feats that seem to happen\n",
      "constantly.\n",
      "One consolation is that since AI is great at information aggregation, it can\n",
      "help us aggregate and summarize all these new updates. But tools can help\n",
      "only to a certain extent. The more overwhelming a space is, the more\n",
      "important it is to have a framework to help us navigate it. This book aims to\n",
      "provide such a framework.\n",
      "The rest of the book will explore this framework step-by-step, starting with\n",
      "\n",
      "Source: {'moddate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'creator': 'calibre 7.4.0', 'page_label': '3', 'creationdate': '2024-12-07T18:13:04+00:00', 'author': 'Chip Huyen', 'producer': 'calibre 7.4.0', 'title': 'AI Engineering (for True Epub)', 'page': 2, 'source': '../data/ai-engineering-book.pdf'}\n",
      "Content: AI Engineering is a practical guide that provides the most up-to-date\n",
      "information on AI development, making it approachable for novice\n",
      "and expert leaders alike. This book is an essential resource for\n",
      "anyone looking to build robust and scalable AI systems.\n",
      "—Vicki Reyzelman, Chief AI Solutions Architect,\n",
      "Mave Sparks\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the provided context, other important terms to know include **AI engineering**, which is the central subject, and **ML engineering**, from which AI engineering has evolved. The context also mentions the **AI engineering stack** as a key component, and the overall goal of building **robust and scalable AI systems** through **AI development**.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Can you give others term beside enthropy that I need to know?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098d95f",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6dd99231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ab9d0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb0d6228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 52.858098947s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is RAG?\n",
      "\n",
      "Once you get the answer, look up how to create that.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 50.768515567s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 46.654437657s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 38.549233776s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (1bf1fff8-dedf-4b52-b7be-016cef12a96a)\n",
      " Call ID: 1bf1fff8-dedf-4b52-b7be-016cef12a96a\n",
      "  Args:\n",
      "    query: What is RAG?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'creator': 'calibre 7.4.0', 'producer': 'calibre 7.4.0', 'title': 'AI Engineering (for True Epub)', 'author': 'Chip Huyen', 'total_pages': 991, 'page_label': '498', 'page': 497, 'source': '../data/ai-engineering-book.pdf', 'moddate': '2024-12-07T18:13:04+00:00', 'creationdate': '2024-12-07T18:13:04+00:00'}\n",
      "Content: RAG Architecture\n",
      "A RAG system has two components: a retriever that retrieves information\n",
      "from external memory sources and a generator that generates a response\n",
      "based on the retrieved information. Figure 6-2 shows a high-level\n",
      "architecture of a RAG system.\n",
      "\n",
      "Source: {'page': 498, 'moddate': '2024-12-07T18:13:04+00:00', 'title': 'AI Engineering (for True Epub)', 'author': 'Chip Huyen', 'creator': 'calibre 7.4.0', 'source': '../data/ai-engineering-book.pdf', 'page_label': '499', 'creationdate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'producer': 'calibre 7.4.0'}\n",
      "Content: Figure 6-2. A basic RAG architecture.\n",
      "In the original RAG paper, Lewis et al. trained the retriever and the\n",
      "generative model together. In today’s RAG systems, these two components\n",
      "are often trained separately, and many teams build their RAG systems using\n",
      "off-the-shelf retrievers and models. However, finetuning the whole RAG\n",
      "system end-to-end can improve its performance significantly.\n",
      "The success of a RAG system depends on the quality of its retriever. A\n",
      "retriever has two main functions: indexing and querying. Indexing involves\n",
      "processing data so that it can be quickly retrieved later. Sending a query to\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "RAG (Retrieval-Augmented Generation) is a system that combines a retriever and a generator. The retriever component fetches information from external memory sources, and the generator then uses this retrieved information to produce a response.\n",
      "Tool Calls:\n",
      "  retrieve (5808f2b3-0fef-483e-9d4c-d28bdc6feeaa)\n",
      " Call ID: 5808f2b3-0fef-483e-9d4c-d28bdc6feeaa\n",
      "  Args:\n",
      "    query: how to create a RAG system\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'producer': 'calibre 7.4.0', 'creator': 'calibre 7.4.0', 'source': '../data/ai-engineering-book.pdf', 'creationdate': '2024-12-07T18:13:04+00:00', 'moddate': '2024-12-07T18:13:04+00:00', 'total_pages': 991, 'page': 497, 'author': 'Chip Huyen', 'title': 'AI Engineering (for True Epub)', 'page_label': '498'}\n",
      "Content: RAG Architecture\n",
      "A RAG system has two components: a retriever that retrieves information\n",
      "from external memory sources and a generator that generates a response\n",
      "based on the retrieved information. Figure 6-2 shows a high-level\n",
      "architecture of a RAG system.\n",
      "\n",
      "Source: {'page_label': '499', 'title': 'AI Engineering (for True Epub)', 'creator': 'calibre 7.4.0', 'producer': 'calibre 7.4.0', 'author': 'Chip Huyen', 'total_pages': 991, 'source': '../data/ai-engineering-book.pdf', 'page': 498, 'creationdate': '2024-12-07T18:13:04+00:00', 'moddate': '2024-12-07T18:13:04+00:00'}\n",
      "Content: Figure 6-2. A basic RAG architecture.\n",
      "In the original RAG paper, Lewis et al. trained the retriever and the\n",
      "generative model together. In today’s RAG systems, these two components\n",
      "are often trained separately, and many teams build their RAG systems using\n",
      "off-the-shelf retrievers and models. However, finetuning the whole RAG\n",
      "system end-to-end can improve its performance significantly.\n",
      "The success of a RAG system depends on the quality of its retriever. A\n",
      "retriever has two main functions: indexing and querying. Indexing involves\n",
      "processing data so that it can be quickly retrieved later. Sending a query to\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "RAG (Retrieval Augmented Generation) is a system composed of two main components: a retriever and a generator. The retriever is responsible for fetching relevant information from external memory sources, and the generator then uses this retrieved information to formulate a response.\n",
      "\n",
      "A basic RAG architecture involves training the retriever and the generative model together. However, in modern RAG systems, these two components are often trained separately. Many teams opt to build their RAG systems using off-the-shelf retrievers and models. Nonetheless, fine-tuning the entire RAG system end-to-end can lead to significant performance improvements.\n",
      "\n",
      "To create a RAG system, the critical component to focus on is the retriever, as its quality determines the system's success. The retriever performs two primary functions:\n",
      "\n",
      "1.  **Indexing:** This involves processing data in a way that allows for rapid retrieval later.\n",
      "2.  **Querying:** This is the process of sending a query to the indexed data to retrieve relevant information.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is RAG?\\n\\n\"\n",
    "    \"Once you get the answer, look up how to create that.\"\n",
    ")\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a8ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-app (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
